{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf4d6fa-f778-479b-80e0-b8c666b5591f",
   "metadata": {},
   "source": [
    "## 基于LangChain和本地的Ollama模型开发一个支持FunctionCall的Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb7a1b-7344-40f4-8304-a2225bd0eaa2",
   "metadata": {},
   "source": [
    "### 环境安装\n",
    "#### Ollama简介\n",
    "Ollama是一个开源的本地大模型执行框架，旨在简化大模型在本地环境的部署和管理过程。\n",
    "#### Ollama本地安装\n",
    "具体指令：`brew install ollama`\n",
    "#### Ollama启动服务\n",
    "具体指令：`ollama serve` or `nohup ollama serve > ollama.log 2>&1 &`\n",
    "#### Ollama运行大模型\n",
    "具体指令：`ollama run {模型key}`  例如：MFDoom/deepseek-r1-tool-calling:1.5b\n",
    "具体可以在"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215554c6-e6a7-454c-a79d-0eb2f22e6d0e",
   "metadata": {},
   "source": [
    "### 样例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7969fa0-2764-4b68-b42e-93fb3229f561",
   "metadata": {},
   "source": [
    "#### 第一步：安装langchain-ollama库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222d20bf-3baa-4fa8-9864-a8db4f8fb217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cfb6cb-5171-4283-9992-1278581c5f8f",
   "metadata": {},
   "source": [
    "#### 第二步：初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a62544-1ad0-42eb-b3bd-a4b7c8d23cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url = 'http://127.0.0.1:11434',\n",
    "    model = 'MFDoom/deepseek-r1-tool-calling:14b',\n",
    "    temperature = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0354bc6-063f-4e78-ae92-cff09d619be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "requestMessages = [\n",
    "    SystemMessage(content = ''),\n",
    "    HumanMessage(content = '')\n",
    "]\n",
    "\n",
    "responseMessage = llm.invoke(requestMessages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
